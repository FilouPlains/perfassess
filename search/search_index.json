{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"\u231b\ufe0f PERFASSESS \ud83d\udcbe <p>\u270d Contributor: Lucas ROUAUD</p> <p>\ud83d\udc31 GitHub repository: https://github.com/FilouPlains/perfassess</p>"},{"location":"#description","title":"\ud83d\udcd2 Description","text":"<p>This module permit to evaluate the performance of a given function and create Plotly bar plot.</p>"},{"location":"#results-example","title":"\ud83d\udcca Results example","text":"<p>Next are Plotly interactive results. These are obtained using next command, in <code>\ud83d\udcc1 perfassess/</code> directory from the GitHub repository:</p> <pre><code>$ perfassess -s perfassess/testor.py \\\\\n             -f testor \\\\\n             -o data/ \\\\\n             -a data/argument.yml\n</code></pre>"},{"location":"#time-evaluation","title":"\u23f3 Time evaluation","text":""},{"location":"#memory-evaluation","title":"\ud83e\udde0 Memory evaluation","text":""},{"location":"#aknowledgement","title":"\ud83d\ude47\u200d\u2642\ufe0f Aknowledgement","text":"<p>\ud83d\udd0d Code reviewing: Hubert Santuz</p> <p>This work is licensed under a MIT License.</p> <p></p>"},{"location":"installation/","title":"\u2699\ufe0f Installation","text":""},{"location":"installation/#ez-way","title":"\ud83e\udd79 EZ way","text":"<p>Recommended method:</p> <pre><code>$ pipx install perfassess\n</code></pre> <p>If you do not have pipx installed, check out the documentation at https://pipx.pypa.io/stable/installation/. Else, classical installation method:</p> <pre><code>$ pip install perfassess\n</code></pre>"},{"location":"installation/#from-source","title":"\ud83d\ude29 From source","text":""},{"location":"installation/#cloning-the-repository","title":"\ud83d\udc6c Cloning the repository","text":"<p>You can clone the repository using <code>HTTPS</code>:</p> <pre><code>$ git clone https://github.com/FilouPlains/perfassess.git\n</code></pre> <p>or using <code>SSH</code>:</p> <pre><code>$ git clone git@github.com:FilouPlains/perfassess.git\n</code></pre> <p>Then go to the <code>\ud83d\udcc1 perfassess/</code> directory:</p> <pre><code>$ cd perfassess\n</code></pre> <p>All next commands are assuming that you are in the <code>\ud83d\udcc1 perfassess/</code> directory. This directory will be name as <code>\ud83d\udcc1 ./</code>.</p>"},{"location":"installation/#installing-with-pip","title":"\ud83d\udc0d\ud83d\udce6 Installing with pip","text":"<p>Simply launch this command in the <code>\ud83d\udcc1 ./</code> directory:</p> <pre><code>$ python3 -m pip install .\n</code></pre> <p>You are now able to launch the program!</p>"},{"location":"code_documentation/class_performance_assessor/","title":"class_performance_assessor.py","text":""},{"location":"code_documentation/class_performance_assessor/#details-about-cprofile-output","title":"Details about cProfile output","text":"Value Description <code>ncalls</code> Shows the number of calls made. <code>tottime</code> Total time taken by the given function. The time made in calls to sub-functions are excluded. <code>percall</code> Total time per numbers of calls. <code>cumtime</code> Like <code>tottime</code>, but includes time spent in all called subfunctions. <code>percall</code> Quotient of <code>cumtime</code> divided by primitive calls. The primitive calls include all calls not included through recursion. <p>Information was collected here: https://www.machinelearningplus.com/python/cprofile-how- to-profile-your-python-code/</p> <p>An object to compute time and memory consumption.</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor","title":"<code>PerformanceAssessor</code>","text":"<p>A class to access the performance of a given function (memory or time).</p> Source code in <code>src/perfassess/class_performance_assessor.py</code> <pre><code>class PerformanceAssessor:\n    \"\"\"A class to access the performance of a given function (memory or time).\n    \"\"\"\n\n    def __init__(\n        self,\n        main: Callable,\n        n_field: int = 0,\n        **kwargs\n    ):\n        \"\"\"Initialize a PerformanceAssessor object.\n\n        Parameters\n        ----------\n        main : `Callable`\n            The function to access.\n\n        n_field : `int`, optional\n            The number of field to keep in function name. By default 9.\n\n            Let us say that we have a function named like this:\n\n            ```sh\n            /home/user/Document/some_function.py:000(function_name)\n            ```\n\n            If input we 2, we will have something like this:\n\n            ```sh\n            Document/some_function.py:000(function_name)\n            ```\n\n            We split at each \"/\" and keep the last two field.\n\n        kwargs\n            All possible arguments for the function to test.\n\n        Example\n        -------\n        ```py\n        assessor: PerformanceAssessor = PerformanceAssessor(\n            main=function_to_test,\n            n_field=1,\n            argument_for_function_to_test=\"some_value\"\n        )\n        ```\n        \"\"\"\n        self.__assessed_function: Callable = main\n        self.__function_argument: dict = dict(kwargs)\n        self.__data: dict = {}\n        self.__plot: \"dict[go.Figure]\" = {}\n        self.__n_field: int = n_field\n\n    def launch_profiling(\n        self,\n        do_memory: bool = True,\n        do_time: bool = True\n    ):\n        \"\"\"Launch the evaluation of performance (memory or time).\n\n        Parameters\n        ----------\n        do_memory : `bool`, optional\n            Do the memory evaluation. By default True.\n\n        do_time : `bool`, optional\n            Do the time evaluation. By default True.\n\n        Raises\n        ------\n        `ValueError`\n            When both `do_memory` and `do_time` are set to `False`.\n        \"\"\"\n        if not do_memory and not do_time:\n            raise ValueError(\"[Err##] One value between \\\"do_memory\\\" or \"\n                             \"\\\"do_time\\\" have to set to `True`.\")\n\n        if do_memory:\n            # Starting to check memory usage.\n            tracemalloc.start()\n\n        if do_time:\n            profile: Profile = Profile()\n            # Starting to check time usage.\n            profile.enable()\n\n        # Launch the function to test.\n        self.__assessed_function(**self.__function_argument)\n\n        if do_memory:\n            # Get a traceback of memory usage.\n            snapshot = tracemalloc.take_snapshot()\n\n            # Create the plot for evaluating memory usage.\n            self.__memory_evaluation(stat_memory=snapshot.statistics(\"lineno\"))\n\n            # Stop to check memory usage.\n            tracemalloc.stop()\n\n        if do_time:\n            # Stop to check time usage.\n            profile.disable()\n\n            # Create the plot for evaluating memory usage.\n            self.__time_evaluation(profile=profile)\n\n    def __memory_evaluation(\n        self,\n        stat_memory: tracemalloc.Statistic\n    ):\n        \"\"\"Parsed memory evaluation output and set a plot.\n\n        Parameters\n        ----------\n        stat_memory : `tracemalloc.Statistic`\n            The \"assessor\".\n        \"\"\"\n        stat_dict = {}\n\n        # Setting the dataset for memory usage.\n        for stat in stat_memory:\n            key: str = str(stat).split()[0]\n\n            if self.__n_field &gt; 0:\n                key = key.split(sep=\"/\")\n                key = key[len(key) - self.__n_field:]\n                key = \"/\".join(key)\n\n            if key not in stat_dict:\n                stat_dict[key] = 0\n\n            stat_dict[key] += stat.size\n\n        # Save data into member.\n        self.__data[\"memory_evaluation\"] = {\n            \"head\": np.array([\"size (Mib)\", \"function\"]),\n            \"label\": np.array(list(stat_dict.keys())),\n            \"data\": np.array([list(stat_dict.values())]).T / 1024\n        }\n\n        # \"Pre-draw\" the plot for time usage.\n        self.__plot[\"memory_evaluation\"] = self.__set_plot(\n            head=np.array([\"size (Mib)\", \"function\"]),\n            label=np.array(list(stat_dict.keys())),\n            data=np.array([list(stat_dict.values())]).T / 1024\n        )\n\n    def __time_evaluation(\n        self,\n        profile: Profile\n    ):\n        \"\"\"Parsed time evaluation output and set a plot.\n\n        Parameters\n        ----------\n        profile : `Profile`\n            The \"assessor\".\n        \"\"\"\n        # Redirect print stdout into a buffer.\n        buffer: StringIO = StringIO()\n        sys.stdout = buffer\n\n        # Get the traceback of time execution.\n        stat_time: Stats = Stats(profile).strip_dirs().print_stats()\n        stat_time = buffer.getvalue()\n\n        # Set the print stdout standard behaviour.\n        sys.stdout = sys.__stdout__\n\n        skip_line: bool = True\n        data_label: np.array = np.array([])\n        numeric_data: np.array = np.array([])\n\n        # Setting the dataset for time usage.\n        for line in stat_time.strip().split(\"\\n\"):\n            if \"ncalls\" in line:\n                data_head: np.array = np.array(line.split())\n                # Add units (seconds) to each header, but the first one.\n                data_head = np.char.add(data_head, [\"\", *[\" (s)\"] * 4, \"\"])\n\n                # pylint: disable=C0103\n                # It is not a constant pylint!\n                skip_line = False\n                # pylint: enable=C0103\n                continue\n\n            if skip_line or \"/\" in line:\n                continue\n\n            line: list = line.split(maxsplit=5)\n\n            data_label = np.append(data_label, line[-1])\n\n            if numeric_data.shape[0] == 0:\n                numeric_data = np.array(line[:-1])\n            else:\n                numeric_data = np.vstack((numeric_data, line[:-1]))\n\n        numeric_data = numeric_data.astype(float)\n\n        # Save data into member.\n        self.__data[\"time_evaluation\"] = {\n            \"head\": data_head,\n            \"label\": data_label,\n            \"data\": numeric_data\n        }\n\n        # \"Pre-draw\" the plot for time usage.\n        self.__plot[\"time_evaluation\"] = self.__set_plot(\n            head=data_head,\n            label=data_label,\n            data=numeric_data\n        )\n\n    # pylint: disable=too-many-arguments, too-many-instance-attributes\n    # Special class that needs a lot of data to be set up. Using a dataclass\n    # would be unnecessary, as far as it would just simply take more memory\n    # for nothing.\n\n    def __set_plot(\n        self,\n        head: np.array,\n        label: np.array,\n        data: np.array,\n        foreground: str = \"#2E2E3E\",\n        background: str = \"rgba(0, 0, 0, 0)\"\n    ) -&gt; go.Figure:\n        \"\"\"Set a Plotly bar plot based on computed evaluation.\n\n        Parameters\n        ----------\n        head : `np.array`\n            The data header (like ncall). Or like one label per column.\n\n        label : `np.array`\n            The data label (like functions names). Or like one label per row.\n\n        data : `np.array`\n            The numerical data.\n\n        foreground : `str`, optional\n            The \"foreground\" color. By default \"#2E2E3E\".\n\n        background : `str`, optional\n            The \"background\" color. By default \"rgba(0, 0, 0, 0)\".\n\n        Returns\n        -------\n        go.Figure\n            The setted Plotly bar plot.\n        \"\"\"\n        plot: object = go.Figure()\n\n        sort_i: np.array = np.flip(np.argsort(data.T[0]))\n\n        # Trace the barplot\n        plot.add_trace(go.Bar(\n            x=label[sort_i],\n            y=data.T[0][sort_i],\n            marker_line_color=foreground,\n            marker_color=foreground\n        ))\n\n        # Modify general plot properties.\n        plot.update_layout(\n            template=\"plotly_white\",\n            margin={\"r\": 5},\n            font={\"size\": 12, \"family\": \"Roboto Light\"},\n            xaxis={\n                \"title\": f\"&lt;b&gt;Tested function ({head[-1]})&lt;/b&gt;\",\n                \"showline\": True,\n                \"linewidth\": 1,\n                \"showgrid\": False,\n                \"title_font\": {\"family\": \"Roboto Black\"},\n                \"tickfont\": {\"size\": 12}\n            },\n            yaxis={\n                \"title\": f\"&lt;b&gt;{head[0].capitalize()}&lt;/b&gt;\",\n                \"showline\": True,\n                \"linewidth\": 1,\n                \"showgrid\": False,\n                \"title_font\": {\"family\": \"Roboto Black\"},\n                \"tickfont\": {\"size\": 12}\n            },\n            title_font={\"family\": \"Roboto Black\"},\n            plot_bgcolor=background,\n            paper_bgcolor=background,\n        )\n\n        # Add the rectangle border.\n        plot.add_shape(\n            type=\"rect\",\n            xref=\"paper\",\n            yref=\"paper\",\n            x0=0,\n            y0=0,\n            x1=1,\n            y1=1,\n            line={\"width\": 2, \"color\": foreground}\n        )\n\n        plot.update_traces()\n\n        # Adding the dropdown menu in the case of multiple datas.\n        if data.T.shape[0] != 1:\n            # Add the dropdown to the plot.\n            plot.update_layout(updatemenus=self.__add_dropdown(\n                foreground=foreground,\n                head=head,\n                label=label,\n                data=data\n            ))\n\n        return plot\n\n    # pylint: enable=too-many-arguments\n\n    def __add_dropdown(\n        self,\n        head: np.array,\n        label: np.array,\n        data: np.array,\n        foreground: str\n    ) -&gt; list:\n        \"\"\"Add a dropdown to the Plotly plot, in order to select different\n        assessed values.\n\n        Parameters\n        ----------\n        head : `np.array`\n            The data header (like ncall). Or like one label per column.\n\n        label : `np.array`\n            The data label (like functions names). Or like one label per row.\n\n        data : `np.array`\n            The numerical data.\n\n        foreground : `str`\n            The \"foreground\" color.\n\n        Returns\n        -------\n        `list`\n            The dropdown menu, which is a `update_menu`.\n        \"\"\"\n        button: list = []\n\n        for i, label_i in enumerate(head[:-1]):\n            sort_i: np.array = np.flip(np.argsort(data.T[i]))\n\n            # Add a element in the dropdown. By selecting it, it will modify\n            # the plot.\n            button += [{\n                \"method\": \"update\",\n                \"label\": label_i,\n                \"args\": [\n                    # Restyling.\n                    {\"x\": [label[sort_i]], \"y\": [data.T[i][sort_i]]},\n                    # Updating.\n                    {\"yaxis\": {\n                        \"showline\": True,\n                        \"linewidth\": 1,\n                        \"showgrid\": False,\n                        \"title\": {\n                            \"text\": f\"&lt;b&gt;{label_i.capitalize()}&lt;/b&gt;\",\n                            \"font\": {\"family\": \"Roboto Black\"}\n                        },\n                        \"tickfont\": {\"size\": 12}\n                    }}\n                ],\n            }]\n\n        # Create the dropdown menu.\n        update_menu: list = [{\n            \"buttons\": button,\n            \"type\": \"dropdown\",\n            \"direction\": \"down\",\n            \"showactive\": True,\n            \"x\": 1,\n            \"xanchor\": \"right\",\n            \"y\": 1.01,\n            \"yanchor\": \"bottom\",\n            \"bgcolor\": \"#FFF\",\n            \"bordercolor\": foreground,\n            \"borderwidth\": 2,\n            \"font_color\": foreground\n        }]\n\n        return update_menu\n\n    def plot(\n        self,\n        path: str = \"./\"\n    ):\n        \"\"\"Save the plot to a `.html` file.\n\n        Parameters\n        ----------\n        path : `str`, optional\n            The path to save the file, which have to be a directory. By default\n            \"./\".\n\n        Raises\n        ------\n        `FileNotFoundError`\n            If the input path does not exist.\n\n        `ValueError`\n            If the input path is not a directory.\n        \"\"\"\n        if path.endswith(\"/\"):\n            path = path[:-1]\n\n        if not exists(path):\n            raise FileNotFoundError(f\"[Err##] Given path \\\"{path}\\\" does not \"\n                                    \"exist.\")\n        if not isdir(path):\n            raise ValueError(f\"[Err##] Given path  \\\"{path}\\\" is not \"\n                             \"directory.\")\n\n        for key, plot_i in self.__plot.items():\n            # Save the plot.\n            plot_i.write_html(\n                file=f\"{path}/{key}.html\",\n                include_plotlyjs=True,\n                full_html=True\n            )\n\n    def get_plot(self) -&gt; dict:\n        \"\"\"Get the setted plot.\n\n        Returns\n        -------\n        `dict`\n            The setted plot.\n        \"\"\"\n        return self.__plot\n\n    def data(self) -&gt; dict:\n        \"\"\"Get computed data.\n\n        Returns\n        -------\n        `dict`\n            Computed data about time or memory usage.\n\n        Raises\n        ------\n        `ValueError`\n            _description_\n        \"\"\"\n        if not self.__data:\n            raise ValueError(\"[Err##] You have to computed properties before\"\n                             \"getting them. For this, use\"\n                             \"`self.launch_profiling()`.\")\n\n        return self.__data\n</code></pre>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.__add_dropdown","title":"<code>__add_dropdown(head, label, data, foreground)</code>","text":"<p>Add a dropdown to the Plotly plot, in order to select different assessed values.</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.__add_dropdown--parameters","title":"Parameters","text":"<p>head : <code>np.array</code>     The data header (like ncall). Or like one label per column.</p> <code>np.array</code> <p>The data label (like functions names). Or like one label per row.</p> <code>np.array</code> <p>The numerical data.</p> <code>str</code> <p>The \"foreground\" color.</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.__add_dropdown--returns","title":"Returns","text":"<p><code>list</code>     The dropdown menu, which is a <code>update_menu</code>.</p> Source code in <code>src/perfassess/class_performance_assessor.py</code> <pre><code>def __add_dropdown(\n    self,\n    head: np.array,\n    label: np.array,\n    data: np.array,\n    foreground: str\n) -&gt; list:\n    \"\"\"Add a dropdown to the Plotly plot, in order to select different\n    assessed values.\n\n    Parameters\n    ----------\n    head : `np.array`\n        The data header (like ncall). Or like one label per column.\n\n    label : `np.array`\n        The data label (like functions names). Or like one label per row.\n\n    data : `np.array`\n        The numerical data.\n\n    foreground : `str`\n        The \"foreground\" color.\n\n    Returns\n    -------\n    `list`\n        The dropdown menu, which is a `update_menu`.\n    \"\"\"\n    button: list = []\n\n    for i, label_i in enumerate(head[:-1]):\n        sort_i: np.array = np.flip(np.argsort(data.T[i]))\n\n        # Add a element in the dropdown. By selecting it, it will modify\n        # the plot.\n        button += [{\n            \"method\": \"update\",\n            \"label\": label_i,\n            \"args\": [\n                # Restyling.\n                {\"x\": [label[sort_i]], \"y\": [data.T[i][sort_i]]},\n                # Updating.\n                {\"yaxis\": {\n                    \"showline\": True,\n                    \"linewidth\": 1,\n                    \"showgrid\": False,\n                    \"title\": {\n                        \"text\": f\"&lt;b&gt;{label_i.capitalize()}&lt;/b&gt;\",\n                        \"font\": {\"family\": \"Roboto Black\"}\n                    },\n                    \"tickfont\": {\"size\": 12}\n                }}\n            ],\n        }]\n\n    # Create the dropdown menu.\n    update_menu: list = [{\n        \"buttons\": button,\n        \"type\": \"dropdown\",\n        \"direction\": \"down\",\n        \"showactive\": True,\n        \"x\": 1,\n        \"xanchor\": \"right\",\n        \"y\": 1.01,\n        \"yanchor\": \"bottom\",\n        \"bgcolor\": \"#FFF\",\n        \"bordercolor\": foreground,\n        \"borderwidth\": 2,\n        \"font_color\": foreground\n    }]\n\n    return update_menu\n</code></pre>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.__init__","title":"<code>__init__(main, n_field=0, **kwargs)</code>","text":"<p>Initialize a PerformanceAssessor object.</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.__init__--parameters","title":"Parameters","text":"<p>main : <code>Callable</code>     The function to access.</p> <code>int</code>, optional <p>The number of field to keep in function name. By default 9.</p> <p>Let us say that we have a function named like this:</p> <pre><code>/home/user/Document/some_function.py:000(function_name)\n</code></pre> <p>If input we 2, we will have something like this:</p> <pre><code>Document/some_function.py:000(function_name)\n</code></pre> <p>We split at each \"/\" and keep the last two field.</p> <p>kwargs     All possible arguments for the function to test.</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.__init__--example","title":"Example","text":"<pre><code>assessor: PerformanceAssessor = PerformanceAssessor(\n    main=function_to_test,\n    n_field=1,\n    argument_for_function_to_test=\"some_value\"\n)\n</code></pre> Source code in <code>src/perfassess/class_performance_assessor.py</code> <pre><code>def __init__(\n    self,\n    main: Callable,\n    n_field: int = 0,\n    **kwargs\n):\n    \"\"\"Initialize a PerformanceAssessor object.\n\n    Parameters\n    ----------\n    main : `Callable`\n        The function to access.\n\n    n_field : `int`, optional\n        The number of field to keep in function name. By default 9.\n\n        Let us say that we have a function named like this:\n\n        ```sh\n        /home/user/Document/some_function.py:000(function_name)\n        ```\n\n        If input we 2, we will have something like this:\n\n        ```sh\n        Document/some_function.py:000(function_name)\n        ```\n\n        We split at each \"/\" and keep the last two field.\n\n    kwargs\n        All possible arguments for the function to test.\n\n    Example\n    -------\n    ```py\n    assessor: PerformanceAssessor = PerformanceAssessor(\n        main=function_to_test,\n        n_field=1,\n        argument_for_function_to_test=\"some_value\"\n    )\n    ```\n    \"\"\"\n    self.__assessed_function: Callable = main\n    self.__function_argument: dict = dict(kwargs)\n    self.__data: dict = {}\n    self.__plot: \"dict[go.Figure]\" = {}\n    self.__n_field: int = n_field\n</code></pre>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.__memory_evaluation","title":"<code>__memory_evaluation(stat_memory)</code>","text":"<p>Parsed memory evaluation output and set a plot.</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.__memory_evaluation--parameters","title":"Parameters","text":"<p>stat_memory : <code>tracemalloc.Statistic</code>     The \"assessor\".</p> Source code in <code>src/perfassess/class_performance_assessor.py</code> <pre><code>def __memory_evaluation(\n    self,\n    stat_memory: tracemalloc.Statistic\n):\n    \"\"\"Parsed memory evaluation output and set a plot.\n\n    Parameters\n    ----------\n    stat_memory : `tracemalloc.Statistic`\n        The \"assessor\".\n    \"\"\"\n    stat_dict = {}\n\n    # Setting the dataset for memory usage.\n    for stat in stat_memory:\n        key: str = str(stat).split()[0]\n\n        if self.__n_field &gt; 0:\n            key = key.split(sep=\"/\")\n            key = key[len(key) - self.__n_field:]\n            key = \"/\".join(key)\n\n        if key not in stat_dict:\n            stat_dict[key] = 0\n\n        stat_dict[key] += stat.size\n\n    # Save data into member.\n    self.__data[\"memory_evaluation\"] = {\n        \"head\": np.array([\"size (Mib)\", \"function\"]),\n        \"label\": np.array(list(stat_dict.keys())),\n        \"data\": np.array([list(stat_dict.values())]).T / 1024\n    }\n\n    # \"Pre-draw\" the plot for time usage.\n    self.__plot[\"memory_evaluation\"] = self.__set_plot(\n        head=np.array([\"size (Mib)\", \"function\"]),\n        label=np.array(list(stat_dict.keys())),\n        data=np.array([list(stat_dict.values())]).T / 1024\n    )\n</code></pre>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.__set_plot","title":"<code>__set_plot(head, label, data, foreground='#2E2E3E', background='rgba(0, 0, 0, 0)')</code>","text":"<p>Set a Plotly bar plot based on computed evaluation.</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.__set_plot--parameters","title":"Parameters","text":"<p>head : <code>np.array</code>     The data header (like ncall). Or like one label per column.</p> <code>np.array</code> <p>The data label (like functions names). Or like one label per row.</p> <code>np.array</code> <p>The numerical data.</p> <code>str</code>, optional <p>The \"foreground\" color. By default \"#2E2E3E\".</p> <code>str</code>, optional <p>The \"background\" color. By default \"rgba(0, 0, 0, 0)\".</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.__set_plot--returns","title":"Returns","text":"<p>go.Figure     The setted Plotly bar plot.</p> Source code in <code>src/perfassess/class_performance_assessor.py</code> <pre><code>def __set_plot(\n    self,\n    head: np.array,\n    label: np.array,\n    data: np.array,\n    foreground: str = \"#2E2E3E\",\n    background: str = \"rgba(0, 0, 0, 0)\"\n) -&gt; go.Figure:\n    \"\"\"Set a Plotly bar plot based on computed evaluation.\n\n    Parameters\n    ----------\n    head : `np.array`\n        The data header (like ncall). Or like one label per column.\n\n    label : `np.array`\n        The data label (like functions names). Or like one label per row.\n\n    data : `np.array`\n        The numerical data.\n\n    foreground : `str`, optional\n        The \"foreground\" color. By default \"#2E2E3E\".\n\n    background : `str`, optional\n        The \"background\" color. By default \"rgba(0, 0, 0, 0)\".\n\n    Returns\n    -------\n    go.Figure\n        The setted Plotly bar plot.\n    \"\"\"\n    plot: object = go.Figure()\n\n    sort_i: np.array = np.flip(np.argsort(data.T[0]))\n\n    # Trace the barplot\n    plot.add_trace(go.Bar(\n        x=label[sort_i],\n        y=data.T[0][sort_i],\n        marker_line_color=foreground,\n        marker_color=foreground\n    ))\n\n    # Modify general plot properties.\n    plot.update_layout(\n        template=\"plotly_white\",\n        margin={\"r\": 5},\n        font={\"size\": 12, \"family\": \"Roboto Light\"},\n        xaxis={\n            \"title\": f\"&lt;b&gt;Tested function ({head[-1]})&lt;/b&gt;\",\n            \"showline\": True,\n            \"linewidth\": 1,\n            \"showgrid\": False,\n            \"title_font\": {\"family\": \"Roboto Black\"},\n            \"tickfont\": {\"size\": 12}\n        },\n        yaxis={\n            \"title\": f\"&lt;b&gt;{head[0].capitalize()}&lt;/b&gt;\",\n            \"showline\": True,\n            \"linewidth\": 1,\n            \"showgrid\": False,\n            \"title_font\": {\"family\": \"Roboto Black\"},\n            \"tickfont\": {\"size\": 12}\n        },\n        title_font={\"family\": \"Roboto Black\"},\n        plot_bgcolor=background,\n        paper_bgcolor=background,\n    )\n\n    # Add the rectangle border.\n    plot.add_shape(\n        type=\"rect\",\n        xref=\"paper\",\n        yref=\"paper\",\n        x0=0,\n        y0=0,\n        x1=1,\n        y1=1,\n        line={\"width\": 2, \"color\": foreground}\n    )\n\n    plot.update_traces()\n\n    # Adding the dropdown menu in the case of multiple datas.\n    if data.T.shape[0] != 1:\n        # Add the dropdown to the plot.\n        plot.update_layout(updatemenus=self.__add_dropdown(\n            foreground=foreground,\n            head=head,\n            label=label,\n            data=data\n        ))\n\n    return plot\n</code></pre>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.__time_evaluation","title":"<code>__time_evaluation(profile)</code>","text":"<p>Parsed time evaluation output and set a plot.</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.__time_evaluation--parameters","title":"Parameters","text":"<p>profile : <code>Profile</code>     The \"assessor\".</p> Source code in <code>src/perfassess/class_performance_assessor.py</code> <pre><code>def __time_evaluation(\n    self,\n    profile: Profile\n):\n    \"\"\"Parsed time evaluation output and set a plot.\n\n    Parameters\n    ----------\n    profile : `Profile`\n        The \"assessor\".\n    \"\"\"\n    # Redirect print stdout into a buffer.\n    buffer: StringIO = StringIO()\n    sys.stdout = buffer\n\n    # Get the traceback of time execution.\n    stat_time: Stats = Stats(profile).strip_dirs().print_stats()\n    stat_time = buffer.getvalue()\n\n    # Set the print stdout standard behaviour.\n    sys.stdout = sys.__stdout__\n\n    skip_line: bool = True\n    data_label: np.array = np.array([])\n    numeric_data: np.array = np.array([])\n\n    # Setting the dataset for time usage.\n    for line in stat_time.strip().split(\"\\n\"):\n        if \"ncalls\" in line:\n            data_head: np.array = np.array(line.split())\n            # Add units (seconds) to each header, but the first one.\n            data_head = np.char.add(data_head, [\"\", *[\" (s)\"] * 4, \"\"])\n\n            # pylint: disable=C0103\n            # It is not a constant pylint!\n            skip_line = False\n            # pylint: enable=C0103\n            continue\n\n        if skip_line or \"/\" in line:\n            continue\n\n        line: list = line.split(maxsplit=5)\n\n        data_label = np.append(data_label, line[-1])\n\n        if numeric_data.shape[0] == 0:\n            numeric_data = np.array(line[:-1])\n        else:\n            numeric_data = np.vstack((numeric_data, line[:-1]))\n\n    numeric_data = numeric_data.astype(float)\n\n    # Save data into member.\n    self.__data[\"time_evaluation\"] = {\n        \"head\": data_head,\n        \"label\": data_label,\n        \"data\": numeric_data\n    }\n\n    # \"Pre-draw\" the plot for time usage.\n    self.__plot[\"time_evaluation\"] = self.__set_plot(\n        head=data_head,\n        label=data_label,\n        data=numeric_data\n    )\n</code></pre>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.data","title":"<code>data()</code>","text":"<p>Get computed data.</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.data--returns","title":"Returns","text":"<p><code>dict</code>     Computed data about time or memory usage.</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.data--raises","title":"Raises","text":"<p><code>ValueError</code> description</p> Source code in <code>src/perfassess/class_performance_assessor.py</code> <pre><code>def data(self) -&gt; dict:\n    \"\"\"Get computed data.\n\n    Returns\n    -------\n    `dict`\n        Computed data about time or memory usage.\n\n    Raises\n    ------\n    `ValueError`\n        _description_\n    \"\"\"\n    if not self.__data:\n        raise ValueError(\"[Err##] You have to computed properties before\"\n                         \"getting them. For this, use\"\n                         \"`self.launch_profiling()`.\")\n\n    return self.__data\n</code></pre>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.get_plot","title":"<code>get_plot()</code>","text":"<p>Get the setted plot.</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.get_plot--returns","title":"Returns","text":"<p><code>dict</code>     The setted plot.</p> Source code in <code>src/perfassess/class_performance_assessor.py</code> <pre><code>def get_plot(self) -&gt; dict:\n    \"\"\"Get the setted plot.\n\n    Returns\n    -------\n    `dict`\n        The setted plot.\n    \"\"\"\n    return self.__plot\n</code></pre>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.launch_profiling","title":"<code>launch_profiling(do_memory=True, do_time=True)</code>","text":"<p>Launch the evaluation of performance (memory or time).</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.launch_profiling--parameters","title":"Parameters","text":"<p>do_memory : <code>bool</code>, optional     Do the memory evaluation. By default True.</p> <code>bool</code>, optional <p>Do the time evaluation. By default True.</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.launch_profiling--raises","title":"Raises","text":"<p><code>ValueError</code>     When both <code>do_memory</code> and <code>do_time</code> are set to <code>False</code>.</p> Source code in <code>src/perfassess/class_performance_assessor.py</code> <pre><code>def launch_profiling(\n    self,\n    do_memory: bool = True,\n    do_time: bool = True\n):\n    \"\"\"Launch the evaluation of performance (memory or time).\n\n    Parameters\n    ----------\n    do_memory : `bool`, optional\n        Do the memory evaluation. By default True.\n\n    do_time : `bool`, optional\n        Do the time evaluation. By default True.\n\n    Raises\n    ------\n    `ValueError`\n        When both `do_memory` and `do_time` are set to `False`.\n    \"\"\"\n    if not do_memory and not do_time:\n        raise ValueError(\"[Err##] One value between \\\"do_memory\\\" or \"\n                         \"\\\"do_time\\\" have to set to `True`.\")\n\n    if do_memory:\n        # Starting to check memory usage.\n        tracemalloc.start()\n\n    if do_time:\n        profile: Profile = Profile()\n        # Starting to check time usage.\n        profile.enable()\n\n    # Launch the function to test.\n    self.__assessed_function(**self.__function_argument)\n\n    if do_memory:\n        # Get a traceback of memory usage.\n        snapshot = tracemalloc.take_snapshot()\n\n        # Create the plot for evaluating memory usage.\n        self.__memory_evaluation(stat_memory=snapshot.statistics(\"lineno\"))\n\n        # Stop to check memory usage.\n        tracemalloc.stop()\n\n    if do_time:\n        # Stop to check time usage.\n        profile.disable()\n\n        # Create the plot for evaluating memory usage.\n        self.__time_evaluation(profile=profile)\n</code></pre>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.plot","title":"<code>plot(path='./')</code>","text":"<p>Save the plot to a <code>.html</code> file.</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.plot--parameters","title":"Parameters","text":"<p>path : <code>str</code>, optional     The path to save the file, which have to be a directory. By default     \"./\".</p>"},{"location":"code_documentation/class_performance_assessor/#src.perfassess.class_performance_assessor.PerformanceAssessor.plot--raises","title":"Raises","text":"<p><code>FileNotFoundError</code>     If the input path does not exist.</p> <p><code>ValueError</code>     If the input path is not a directory.</p> Source code in <code>src/perfassess/class_performance_assessor.py</code> <pre><code>def plot(\n    self,\n    path: str = \"./\"\n):\n    \"\"\"Save the plot to a `.html` file.\n\n    Parameters\n    ----------\n    path : `str`, optional\n        The path to save the file, which have to be a directory. By default\n        \"./\".\n\n    Raises\n    ------\n    `FileNotFoundError`\n        If the input path does not exist.\n\n    `ValueError`\n        If the input path is not a directory.\n    \"\"\"\n    if path.endswith(\"/\"):\n        path = path[:-1]\n\n    if not exists(path):\n        raise FileNotFoundError(f\"[Err##] Given path \\\"{path}\\\" does not \"\n                                \"exist.\")\n    if not isdir(path):\n        raise ValueError(f\"[Err##] Given path  \\\"{path}\\\" is not \"\n                         \"directory.\")\n\n    for key, plot_i in self.__plot.items():\n        # Save the plot.\n        plot_i.write_html(\n            file=f\"{path}/{key}.html\",\n            include_plotlyjs=True,\n            full_html=True\n        )\n</code></pre>"},{"location":"code_documentation/main/","title":"main.py","text":"<p>Compute time and memory consumption.</p>"},{"location":"code_documentation/main/#src.perfassess.main--linting","title":"Linting","text":"<pre><code>$ pylint src/\n</code></pre>"},{"location":"code_documentation/main/#src.perfassess.main--usage","title":"Usage","text":"<p>You can go on: https://github.com/FilouPlains/perfassess</p> <p>To check how to install this module. After, you can launch the module as a command line. To know what is possible, launch: <pre><code>$ perfassess --help\n</code></pre></p> <p>Else, you are also able to import the module object as followed:</p> <pre><code>from perfassess.class_performance_assessor import PerformanceAssessor\nfrom perfassess.testor import testor\n\nassessor: PerformanceAssessor = PerformanceAssessor(\n    main=testor,\n    n_field=1,\n    value=[0] * 1000\n)\n\nassessor.launch_profiling()\nassessor.plot(path=\"output_directory/\")\n</code></pre> <p>With \"output_directory/\" being the output directory of your choice</p>"},{"location":"code_documentation/main/#src.perfassess.main.main","title":"<code>main()</code>","text":"<p>Main function.</p> Source code in <code>src/perfassess/main.py</code> <pre><code>def main():\n    \"\"\"Main function.\n    \"\"\"\n    __argument = parse_argument(version=__version__)\n\n    assessor: PerformanceAssessor = PerformanceAssessor(\n        main=__argument.function,\n        n_field=__argument.n_field,\n        **__argument.argument\n    )\n\n    assessor.launch_profiling()\n    assessor.plot(path=__argument.output)\n</code></pre>"},{"location":"code_documentation/testor/","title":"testor.py","text":"<p>A simple function to test the module.</p>"},{"location":"code_documentation/testor/#src.perfassess.testor.testor","title":"<code>testor(value, to_add=1)</code>","text":"<p>A function to launch the program in order to test it.</p>"},{"location":"code_documentation/testor/#src.perfassess.testor.testor--parameters","title":"Parameters","text":"<p>value : <code>list</code>     A list of values.</p> <code>int</code>, optional <p>An optional argument to test them. By default 1.</p> Source code in <code>src/perfassess/testor.py</code> <pre><code>def testor(value: list, to_add: int = 1):\n    \"\"\"A function to launch the program in order to test it.\n\n    Parameters\n    ----------\n    value : `list`\n        A list of values.\n\n    to_add : `int`, optional\n        An optional argument to test them. By default 1.\n    \"\"\"\n    for value_i in value:\n        to_add += value_i\n</code></pre>"},{"location":"code_documentation/parse_argument/check_argument/","title":"check_argument.py","text":"<p>Contains a function to parse given arguments.</p>"},{"location":"code_documentation/parse_argument/check_argument/#src.perfassess.parse_argument.check_argument.__file_errors","title":"<code>__file_errors(argument)</code>","text":"<p>Check errors linked to given files.</p>"},{"location":"code_documentation/parse_argument/check_argument/#src.perfassess.parse_argument.check_argument.__file_errors--parameters","title":"Parameters","text":"<p>argument : <code>ArgumentParser</code>     The parsed argument to check.</p>"},{"location":"code_documentation/parse_argument/check_argument/#src.perfassess.parse_argument.check_argument.__file_errors--raises","title":"Raises","text":"<p><code>FileNotFoundError</code>     If the script file is not found.</p> <p><code>ValueError</code>     If the script file have not a \".py\" extension.</p> <p><code>FileNotFoundError</code>     If the package file is not found.</p> <p><code>ValueError</code>     If the package file is not a \"init.py\" one.</p> <p><code>FileNotFoundError</code>     If the subpackage file is not found.</p> <p><code>ValueError</code>     If the package file is not a \"init.py\" one.</p> <p><code>ValueError</code>     If the package file is not a \"init.py\" one.</p> <p><code>FileNotFoundError</code>     If the YAML file is not found.</p> <p><code>ValueError</code>     If the YAML file have not a \".yml\" extension.</p> <p><code>FileNotFoundError</code>     If the directory is not found.</p> <p><code>ValueError</code>     If the path given is not a directory.</p> <p><code>ValueError</code>     If a number of negative file is given.</p> Source code in <code>src/perfassess/parse_argument/check_argument.py</code> <pre><code>def __file_errors(argument: object):\n    \"\"\"Check errors linked to given files.\n\n    Parameters\n    ----------\n    argument : `ArgumentParser`\n        The parsed argument to check.\n\n    Raises\n    ------\n    `FileNotFoundError`\n        If the script file is not found.\n\n    `ValueError`\n        If the script file have not a \".py\" extension.\n\n    `FileNotFoundError`\n        If the package file is not found.\n\n    `ValueError`\n        If the package file is not a \"__init__.py\" one.\n\n    `FileNotFoundError`\n        If the subpackage file is not found.\n\n    `ValueError`\n        If the package file is not a \"__init__.py\" one.\n\n    `ValueError`\n        If the package file is not a \"__init__.py\" one.\n\n    `FileNotFoundError`\n        If the YAML file is not found.\n\n    `ValueError`\n        If the YAML file have not a \".yml\" extension.\n\n    `FileNotFoundError`\n        If the directory is not found.\n\n    `ValueError`\n        If the path given is not a directory.\n\n    `ValueError`\n        If a number of negative file is given.\n    \"\"\"\n    # Input script errors.\n    if not exists(argument.script):\n        raise FileNotFoundError(f\"[Err##] In script, file \\\"{argument.script}\\\" \"\n                                \"does not exist.\")\n    if not argument.script.endswith(\".py\"):\n        raise ValueError(f\"[Err##] In script, file \\\"{argument.script}\\\" have \"\n                         \"not the extension \\\".py\\\".\")\n\n    # Input package errors.\n    if argument.package is not None:\n        if not exists(argument.package):\n            raise FileNotFoundError(\"[Err##] In script, file \"\n                                    f\"\\\"{argument.package}\\\" does not exist.\")\n        if not argument.package.endswith(\"__init__.py\"):\n            raise ValueError(f\"[Err##] In script, file \\\"{argument.package}\\\" \"\n                             \"have to be a \\\"__init__.py\\\" file.\")\n\n    # Input subpackage errors.\n    if argument.subpackage is not None:\n        if not exists(argument.subpackage):\n            raise FileNotFoundError(\"[Err##] In script, file \"\n                                    f\"\\\"{argument.subpackage}\\\" does not \"\n                                    \"exist.\")\n        if not argument.subpackage.endswith(\"__init__.py\"):\n            raise ValueError(\"[Err##] In script, file \"\n                             f\"\\\"{argument.subpackage}\\\" have to be a \"\n                             \"\\\"__init__.py\\\" file.\")\n\n    # Input YAML errors.\n    if argument.argument is not None:\n        if not exists(argument.argument):\n            raise FileNotFoundError(\"[Err##] In script, file \"\n                                    f\"\\\"{argument.argument}\\\" does not \"\n                                    \"exist.\")\n        if not argument.argument.endswith(\".yml\"):\n            raise ValueError(\"[Err##] In script, file \"\n                             f\"\\\"{argument.argument}\\\" have to be a \"\n                             \"\\\".yml\\\" file.\")\n\n    # Deleting potential \"/\" at directory end.\n    if argument.output[-1] == \"/\":\n        argument.output = argument.output[:-1]\n\n    # Output errors.\n    if not exists(argument.output):\n        raise FileNotFoundError(\"[Err##] In output, directory \"\n                                f\"\\\"{argument.output}\\\" does not exist.\")\n    if not isdir(argument.output):\n        raise ValueError(\"[Err##] In output, the given path is not a \"\n                         \"directory.\")\n\n    if argument.n_field &lt; 0:\n        raise ValueError(\"[Err##] In n_filed, the value \"\n                         f\"\\\"{argument.n_field}\\\" should be greater strictly \"\n                         \"to 0.\")\n</code></pre>"},{"location":"code_documentation/parse_argument/check_argument/#src.perfassess.parse_argument.check_argument.__module_importation_error","title":"<code>__module_importation_error(argument)</code>","text":"<p>Try to import the given file in a module and do some modifications.</p>"},{"location":"code_documentation/parse_argument/check_argument/#src.perfassess.parse_argument.check_argument.__module_importation_error--parameters","title":"Parameters","text":"<p>argument : <code>ArgumentParser</code>     The parsed argument to check.</p>"},{"location":"code_documentation/parse_argument/check_argument/#src.perfassess.parse_argument.check_argument.__module_importation_error--returns","title":"Returns","text":"<p><code>ArgumentParser</code>     The modified and corrected parsed arguments.</p>"},{"location":"code_documentation/parse_argument/check_argument/#src.perfassess.parse_argument.check_argument.__module_importation_error--raises","title":"Raises","text":"<p><code>ValueError</code>     If subpackage is filled, package must be given too.</p> <p><code>ValueError</code>     The given function name is not found in the given module.</p> Source code in <code>src/perfassess/parse_argument/check_argument.py</code> <pre><code>def __module_importation_error(argument: object) -&gt; object:\n    \"\"\"Try to import the given file in a module and do some modifications.\n\n    Parameters\n    ----------\n    argument : `ArgumentParser`\n        The parsed argument to check.\n\n    Returns\n    -------\n    `ArgumentParser`\n        The modified and corrected parsed arguments.\n\n    Raises\n    ------\n    `ValueError`\n        If subpackage is filled, package must be given too.\n\n    `ValueError`\n        The given function name is not found in the given module.\n    \"\"\"\n    # =============\n    #\n    # SIMPLE SCRIPT\n    #\n    # =============\n    if argument.package is None and argument.subpackage is None:\n        # Load the function inside the module.\n        module_spec = spec_from_file_location(\n            name=argument.function,\n            location=argument.script\n        )\n\n        # \"Prepare\" the module.\n        module = module_from_spec(spec=module_spec)\n\n        # \"Launch\" the module inside the python environment.\n        module_spec.loader.exec_module(module=module)\n    # =======\n    #\n    # PACKAGE\n    #\n    # =======\n    elif argument.package is not None and argument.subpackage is None:\n        # Get the package name.\n        package_name: str = argument.package.split(\"/\")[-2]\n        # Load the package.\n        package_spec = spec_from_file_location(package_name, argument.package)\n\n        # \"Prepare\" the package.\n        package = module_from_spec(package_spec)\n        # Add the package to the modules dict.\n        modules[package_name] = package\n\n        # \"Launch\" the package.\n        package_spec.loader.exec_module(package)\n\n        # Set the module name.\n        module_name: list = []\n\n        for item in argument.script[:-3].split(\"/\")[::-1]:\n            module_name += [item]\n\n            if item == package_name:\n                break\n\n        module_name: str = \".\".join(module_name[::-1])\n\n        # Load the script inside the module.\n        module_spec = spec_from_file_location(module_name, argument.script)\n\n        # \"Prepare\" the script.\n        module = module_from_spec(module_spec)\n        # Add the script to the modules dict.\n        modules[package_name + \".None\"] = module\n\n        # \"Launch\" the script.\n        module_spec.loader.exec_module(module)\n    # ==========\n    #\n    # SUBPACKAGE\n    #\n    # ==========\n    elif argument.package is not None and argument.subpackage is not None:\n        # Get the package name.\n        package_name: str = argument.package.split(\"/\")[-2]\n        # Load the package.\n        package_spec = spec_from_file_location(package_name, argument.package)\n\n        # \"Prepare\" the package.\n        package = module_from_spec(package_spec)\n        # Add the package to the modules dict.\n        modules[package_name] = package\n\n        # \"Launch\" the package.\n        package_spec.loader.exec_module(package)\n\n        # Set the module name.\n        module_name: list = []\n\n        for item in argument.script[:-3].split(\"/\")[::-1]:\n            module_name += [item]\n\n            if item == package_name:\n                break\n\n        module_name: str = \".\".join(module_name[::-1])\n\n        # Set the subpackage name.\n        subpackage_name: list = []\n\n        for item in argument.subpackage[:-3].split(\"/\")[::-1]:\n            subpackage_name += [item]\n\n            if item == package_name:\n                break\n\n        subpackage_name: str = \".\".join(subpackage_name[::-1])\n\n        # Load the script inside the module.\n        module_spec = spec_from_file_location(module_name, argument.script)\n\n        # \"Prepare\" the script.\n        module = module_from_spec(module_spec)\n        # Add the script to the modules dict.\n        modules[subpackage_name] = module\n\n        # \"Launch\" the script.\n        module_spec.loader.exec_module(module)\n    else:\n        raise ValueError(\"[Err##] If --subpackage is specified, --package \"\n                         \"must be specified too.\")\n\n    # Check if the function exists inside the module/package/subpackage.\n    try:\n        argument.function = module.__dict__[argument.function]\n    except KeyError as error:\n        raise ValueError(f\"[Err##] \\\"{argument.function}\\\" is not present \"\n                         \"inside the given script \"\n                         f\"\\\"{argument.script}\\\".\") from error\n\n    return argument\n</code></pre>"},{"location":"code_documentation/parse_argument/check_argument/#src.perfassess.parse_argument.check_argument.check_argument","title":"<code>check_argument(argument)</code>","text":"<p>Check if the different given arguments are good or not.</p>"},{"location":"code_documentation/parse_argument/check_argument/#src.perfassess.parse_argument.check_argument.check_argument--parameters","title":"Parameters","text":"<p>argument : <code>ArgumentParser</code>     The parsed arguments to checked.</p>"},{"location":"code_documentation/parse_argument/check_argument/#src.perfassess.parse_argument.check_argument.check_argument--returns","title":"Returns","text":"<p><code>ArgumentParser</code>     The modified and corrected parsed arguments.</p> Source code in <code>src/perfassess/parse_argument/check_argument.py</code> <pre><code>def check_argument(argument: object) -&gt; object:\n    \"\"\"Check if the different given arguments are good or not.\n\n    Parameters\n    ----------\n    argument : `ArgumentParser`\n        The parsed arguments to checked.\n\n    Returns\n    -------\n    `ArgumentParser`\n        The modified and corrected parsed arguments.\n    \"\"\"\n    # Check errors linked to given files.\n    __file_errors(argument=argument)\n    # Check errors linked to module importations.\n    argument = __module_importation_error(argument=argument)\n\n    # Parse the \".yml\" file.\n    if argument.argument is not None:\n        # Parsing user file.\n        with open(argument.argument, \"r\", encoding=\"utf-8\") as file:\n            argument.argument = safe_load(file)\n    else:\n        argument.argument = {}\n\n    return argument\n</code></pre>"},{"location":"code_documentation/parse_argument/define_argument/","title":"define_argument.py","text":"<p>Contains a function to parse given arguments.</p>"},{"location":"code_documentation/parse_argument/define_argument/#src.perfassess.parse_argument.define_argument.define_argument","title":"<code>define_argument(version=None)</code>","text":"<p>Parse user given arguments.</p>"},{"location":"code_documentation/parse_argument/define_argument/#src.perfassess.parse_argument.define_argument.define_argument--parameters","title":"Parameters","text":"<p>version : <code>str</code>, optional     The script version. By default None.</p>"},{"location":"code_documentation/parse_argument/define_argument/#src.perfassess.parse_argument.define_argument.define_argument--returns","title":"Returns","text":"<p><code>ArgumentParser</code>     The object with unchecked parsed arguments.</p> Source code in <code>src/perfassess/parse_argument/define_argument.py</code> <pre><code>def define_argument(version: str = None) -&gt; ArgumentParser:\n    \"\"\"Parse user given arguments.\n\n    Parameters\n    ----------\n    version : `str`, optional\n        The script version. By default None.\n\n    Returns\n    -------\n    `ArgumentParser`\n        The object with unchecked parsed arguments.\n    \"\"\"\n    logo: str = \"\"\"\n                \u2584\u2580\u2580\u2588\u2584   \u2584\u2580\u2580\u2580\u2580\u2584  \u2584\u2580\u2580\u2580\u2580\u2584  \u2584\u2580\u2580\u2588\u2584\u2584\u2584\u2584  \u2584\u2580\u2580\u2580\u2580\u2584  \u2584\u2580\u2580\u2580\u2580\u2584\n               \u2590 \u2584\u2580 \u2580\u2584 \u2588 \u2588   \u2590 \u2588 \u2588   \u2590 \u2590  \u2584\u2580   \u2590 \u2588 \u2588   \u2590 \u2588 \u2588   \u2590\n                 \u2588\u2584\u2584\u2584\u2588    \u2580\u2584      \u2580\u2584     \u2588\u2584\u2584\u2584\u2584\u2584     \u2580\u2584      \u2580\u2584  \n                \u2584\u2580   \u2588 \u2580\u2584   \u2588  \u2580\u2584   \u2588    \u2588    \u258c  \u2580\u2584   \u2588  \u2580\u2584   \u2588 \n               \u2588   \u2584\u2580   \u2588\u2580\u2580\u2580    \u2588\u2580\u2580\u2580    \u2584\u2580\u2584\u2584\u2584\u2584    \u2588\u2580\u2580\u2580    \u2588\u2580\u2580\u2580 \u2584\n               \u2590   \u2590    \u2590       \u2590       \u2588    \u2590    \u2590       \u2590     \n                                        \u2590                        \n                     _____________________________________\n                    |\"\"|\"\"|\"\"|\"\"|\"\"|\"\"|\"\"|\"\"|\"\"|\"\"|\"\"|\"\"|\"|\n                    |  1  2  3  4  5  6  7  8  9 10 11 12 |\n                    '-------------------------------------'\n    \"\"\"\n\n    print(logo[1:])\n\n    # Description of the program given when the help is cast.\n    description: str = \"\"\"\n    \"Assess.\" is a program to assess the time execution of a given script in\n    python. To use it, launch:\n\n    \\033[7m [[NORMAL USE]] \\033[0m\\n\n\n        $ perfassess -s script.py \\\\\n                     -f function_name \\\\\n                     -o output_directory/ \\\\\n                     -a argument.yml\n\n    You can actually directly test the program in the repository root\n    (perfassess/) using:\n\n        $ perfassess -s src/perfassess/testor.py \\\\\n                     -f testor \\\\\n                     -a data/argument.yml \\\\\n                     -o data/\n\n    \\033[7m [[PACKAGE USE]] \\033[0m\\n\n    Let us say that you want to test a package, which should have this kind of\n    tree structure:\n\n        package/\n        \u2514\u2500\u2500 src/\n            \u251c\u2500\u2500 __init__.py\n            \u2514\u2500\u2500 script.py\n\n    In package, you can use relative paths. But with these, the \"normal use\"\n    method do not work. Instead, use the next command, if you are in package/,\n    in order to evaluate script.py:\n\n        $ perfassess -s src/script.py \\\\\n                     -f function_name \\\\\n                     -o output_directory/ \\\\\n                     --package src/__init__.py \\\\\n                     -a argument.yml\n\n    In addition of the previous \"normal use\" method, you have to indicate a\n    __init__.py file.\n\n    \\033[7m [[SUBACKAGE USE]] \\033[0m\\n\n    Let us say that you want to test a subpackage, which should have this kind\n    of tree structure:\n\n        ./package/\n        \u2514\u2500\u2500 src/\n            \u251c\u2500\u2500 __init__.py\n            \u2514\u2500\u2500 subpackage/\n                \u251c\u2500\u2500 __init__.py\n                \u2514\u2500\u2500 script.py\n\n    In subpackage, you can use relative paths. But with these, the \"normal use\"\n    method do not work. But the \"package use\" also do not work. Instead, use\n    the next command, if you are in package/, in order to evaluate script.py:\n\n        $ perfassess -s src/subpackage/script.py \\\\\n                     -f function_name \\\\\n                     -o output_directory/ \\\\\n                     --package src/__init__.py \\\\\n                     --subpackage src/subpackage/__init__.py \\\\\n                     -a argument.yml\n\n    In addition of the previous \"normal use\" and \"package use\" method, you have\n    to indicate two __init__.py files. The first one is the top package\n    __init__.py file. The second one is the __init__.py file of the subpackage\n    to test.\n    \"\"\"\n\n    # ===================\n    #\n    # ADD ARGUMENT PARSER\n    #\n    # ===================\n\n    # Setup the arguments parser object.\n    parser: object = ArgumentParser(\n        description=dedent(description)[1:-1],\n        formatter_class=RawTextHelpFormatter,\n        add_help=False\n    )\n\n    # == REQUIRED.\n    parser.add_argument(\n        \"-s\",\n        \"--script\",\n        dest=\"script\",\n        required=True,\n        type=str,\n        metavar=\"[FILE][\\\".py\\\"]\",\n        help=(\"\\033[7m [[MANDATORY]] \\033[0m\\n    &gt; The \\\"scrit.py\\\" file to \"\n              \"assess.\")\n    )\n\n    parser.add_argument(\n        \"-o\",\n        \"--output\",\n        dest=\"output\",\n        required=True,\n        type=str,\n        metavar=\"[DIRECTORY]\",\n        help=(\"\\033[7m [[MANDATORY]] \\033[0m\\n    &gt; A folder where the \"\n              \"plots will be stored.\")\n    )\n\n    # == OPTIONAL.\n    parser.add_argument(\n        \"-h\",\n        \"--help\",\n        action=\"help\",\n        help=\"    &gt; Display this help message, then exit the program.\"\n    )\n\n    parser.add_argument(\n        \"-v\",\n        \"--version\",\n        action=\"version\",\n        version=f\"Program version is {version}\",\n        help=\"    &gt; Display the program's version, then exit the\\nprogram.\"\n    )\n\n    parser.add_argument(\n        \"-f\",\n        \"--function\",\n        dest=\"function\",\n        required=False,\n        default=\"main\",\n        type=str,\n        metavar=\"[str|main]\",\n        help=(\"    &gt; The entry point to assess a script. It have to be\\nthe \"\n              \"name of a function. By default, \\\"main\\\".\")\n    )\n\n    parser.add_argument(\n        \"--n_field\",\n        dest=\"n_field\",\n        required=False,\n        default=0,\n        type=int,\n        metavar=\"[int|0]\",\n        help=(\"    &gt; The number of field to keep in function name. \"\n              \"By\\ndefault 0.\")\n    )\n\n    parser.add_argument(\n        \"--package\",\n        dest=\"package\",\n        required=False,\n        default=None,\n        type=str,\n        metavar=\"[FILE][\\\".py\\\"]\",\n        help=\"    &gt; The path to an \\\"__init__.py\\\" package. By default\\nNone.\"\n    )\n\n    parser.add_argument(\n        \"--subpackage\",\n        dest=\"subpackage\",\n        required=False,\n        default=None,\n        type=str,\n        metavar=\"[FILE][\\\".py\\\"]\",\n        help=(\"    &gt; The path to an \\\"__init__.py\\\" subpackage. By\\ndefault \"\n              \"None.\")\n    )\n\n    parser.add_argument(\n        \"-a\",\n        \"--argument\",\n        dest=\"argument\",\n        required=False,\n        default=None,\n        type=None,\n        metavar=\"[FILE][\\\".yml\\\"]\",\n        help=(\"    &gt; A YAML file containing all argument for the\\nfunction to \"\n              \"test. By default None.\")\n    )\n\n    argument: ArgumentParser = parser.parse_args()\n\n    return argument\n</code></pre>"},{"location":"code_documentation/parse_argument/parse_argument/","title":"parse_argument.py","text":"<p>Contains a function to parse given arguments.</p>"},{"location":"code_documentation/parse_argument/parse_argument/#src.perfassess.parse_argument.parse_argument.parse_argument","title":"<code>parse_argument(version=None)</code>","text":"<p>Parse given arguments and tests them.</p>"},{"location":"code_documentation/parse_argument/parse_argument/#src.perfassess.parse_argument.parse_argument.parse_argument--parameters","title":"Parameters","text":"<p>version : <code>str</code>, optional     The script version. By default None.</p>"},{"location":"code_documentation/parse_argument/parse_argument/#src.perfassess.parse_argument.parse_argument.parse_argument--returns","title":"Returns","text":"<p><code>ArgumentParser</code>     The object with checked parsed arguments.</p> Source code in <code>src/perfassess/parse_argument/parse_argument.py</code> <pre><code>def parse_argument(version: str = None) -&gt; object:\n    \"\"\"Parse given arguments and tests them.\n\n    Parameters\n    ----------\n    version : `str`, optional\n        The script version. By default None.\n\n    Returns\n    -------\n    `ArgumentParser`\n        The object with checked parsed arguments.\n    \"\"\"\n    # Parse the arguments.\n    argument = define_argument(version=version)\n    # Test the arguments.\n    argument = check_argument(argument)\n\n    return argument\n</code></pre>"},{"location":"usage/command_line/","title":"\u2328\ufe0f Using command line","text":""},{"location":"usage/command_line/#general-note","title":"\u270f\ufe0f General note","text":"<p>By doing:</p> <pre><code>$ perfassess --help\n</code></pre> <p>You will get the help to use the command line interface. Here are the used legends:</p> <ul> <li><code>int</code>: Integer.</li> <li><code>str</code>: String.</li> <li><code>[type|value]</code>: Type of the input required, follow by the default value. So if this optional arguments is not used, \u201cvalue\u201d will be chosen.</li> </ul> <p>\u26a0\ufe0f If you use <code>pickle</code>, the command line interface will not work!</p>"},{"location":"usage/command_line/#normal-use","title":"\ud83d\udcc4 Normal use","text":"<p>To use th program, launch:</p> <pre><code>$ perfassess -s script.py \\\\\n             -f function_name \\\\\n             -o output_directory/ \\\\\n             -a argument.yml\n</code></pre> <p>You can actually directly test the program in the repository root (<code>\ud83d\udcc1 perfassess/</code>) using:</p> <pre><code>$ perfassess -s src/perfassess/testor.py \\\\\n             -f testor\n             -a data/argument.yml \\\\\n             -o data/\n</code></pre>"},{"location":"usage/command_line/#package-use","title":"\ud83d\udcc1 Package use","text":"<p>Let us say that you want to test a package, which should have this kind of tree structure:</p> <pre><code>package/\n\u2514\u2500\u2500 src/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 script.py\n</code></pre> <p>In package, you can use relative paths. But with these, the \u201cnormal use\u201d method do not work. Instead, use the next command, if you are in <code>\ud83d\udcc1 package/</code>, in order to evaluate <code>script.py</code>:</p> <pre><code>$ perfassess -s src/script.py \\\\\n             -f function_name \\\\\n             -o output_directory/ \\\\\n             --package src/__init__.py \\\\\n             -a argument.yml\n</code></pre> <p>In addition of the previous \"normal use\" method, you have to indicate a <code>__init__.py</code> file.</p> <p>Note</p> <p>Packages are identify with <code>__init__.py</code> files and allow relatives import.</p>"},{"location":"usage/command_line/#subpackage-use","title":"\ud83d\uddc2 Subpackage use","text":"<p>Let us say that you want to test a subpackage, which should have this kind of tree structure:</p> <pre><code>./package/\n\u2514\u2500\u2500 src/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 subpackage/\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 script.py\n</code></pre> <p>In subpackage, you can use relative paths. But with these, the \u201cnormal use\u201d method do not work. But the \u201cpackage use\u201d also do not work. Instead, use the next command, if you are in <code>\ud83d\udcc1 package/</code>, in order to evaluate <code>script.py</code>:</p> <pre><code>$ perfassess -s src/subpackage/script.py \\\\\n             -f function_name \\\\\n             -o output_directory/ \\\\\n             --package src/__init__.py \\\\\n             --subpackage src/subpackage/__init__.py \\\\\n             -a argument.yml\n</code></pre> <p>In addition of the previous \u201cnormal use\u201d and \u201cpackage use\u201d method, you have to indicate two <code>__init__.py</code> files. The first one is the top package <code>__init__.py</code> file. The second one is the <code>__init__.py</code> file of the subpackage to test.</p> <p>Note</p> <p>Packages are identify with <code>__init__.py</code> files and allow relatives import.</p>"},{"location":"usage/command_line/#describing-possible-parameters","title":"\ud83d\udd0d Describing possible parameters","text":"Argument Mandatory? Type and usage Description <code>-s</code><code>--script</code> Yes <code>-s script.py</code> The script that contain the function to test. <code>-o</code><code>--output</code> Yes <code>-o output_directory/</code> The directory where the plot have to be produced. <code>-f</code><code>--function</code> No <code>-f main</code> The function to test. <code>-a</code><code>--argument</code> No <code>-a argument.yml</code> The argument to passe to the function to test*. <code>--n_field</code> No <code>-n_field 2</code> The number of field to keep**. <code>--package</code> No <code>--package package/__init__.py</code> The <code>__init__.py</code> file of the top package to test. <code>--subpackage</code> No <code>-o package/subpackage/__init__.py</code> The <code>__init__.py</code> file of the subpackage to test. <code>-h</code><code>--help</code> No Flag Display the help and exit the program. <code>-v</code><code>--version</code> No Flag Display the version and exit the program. <ul> <li>* = If you want to test a function that requires arguments, you can give to the command line interface a <code>argument.yml</code> file that contains all required arguments. If the tested function requires an argument like <code>toto</code>, you can put in the YAML file:</li> </ul> <pre><code>toto: 10\n</code></pre> <ul> <li>** = In memory usage, tested functions are going to be named something like <code>/home/user/Documents/program/package/script.py</code>. To shorten the name, use the <code>--n_field</code> tag. For instance, giving 2 will let know to the program that you want to only keep the last two field, which in the end will look something like: <code>package/script.py</code>.</li> </ul>"},{"location":"usage/command_line/#yaml-file-example","title":"\ud83d\uddd2 YAML file example","text":"<p>For the next python function, describe here:</p> <pre><code>def testor(value: list, to_add: int = 1):\n    \"\"\"A function to launch the program in order to test it.\n\n    Parameters\n    ----------\n    value : `list`\n        A list of values.\n\n    to_add : `int`, optional\n        An optional argument to test them. By default 1.\n    \"\"\"\n    for value_i in value:\n        to_add += value_i\n</code></pre> <p>You can see that the function have:</p> <ul> <li>An mandatory argument <code>value</code>: it is a list of value.</li> <li>An optional argument <code>to_add</code>: it is an integer with the default value of 1.</li> </ul> <p>That means that you need a way to provide the arguments to the function when you are going to use the command line interface. The chosen method is a YAML file. An example of a file, available here in the GitHub repository, is the following:</p> <pre><code>value: [1, 2, 3]\n</code></pre> <p>That means that, when launching the command line, you are going to give to value a list that look like this: <code>[1, 2, 3]</code>. Translate in python code, it would be something like:</p> <pre><code>testor(value = [1, 2, 3])\n</code></pre> <p>As you can see, only the mandatory argument is given. You do not need to specify anything for the optional ones. If you want to, an other example of YAML file could look to something like this:</p> <pre><code>value: [1, 2, 3]\nto_add: 100\n</code></pre> <p>To finish, you can actually directly test the program in the repository root (<code>\ud83d\udcc1 perfassess/</code>) using:</p> <pre><code>$ perfassess -s src/perfassess/testor.py \\\\\n             -f testor \\\\\n             -a data/argument.yml \\\\\n             -o data/\n</code></pre> <p>Which is an example of launching the command giving arguments to the function <code>testor()</code> with a YAML file. If you function do not need any arguments, then the command could look to something like this:</p> <pre><code>$ perfassess -s src/perfassess/testor.py \\\\\n             -f testor \\\\\n             -o data/\n</code></pre> <p>Note</p> <p>In the background, <code>pyyaml</code> is used to parse the file and translate it into a dictionary. Then, we can pass it to the wanted function using <code>**kwargs</code> python unpacking \u201cmethod\u201d.</p>"},{"location":"usage/module/","title":"\ud83d\udce6 Module","text":""},{"location":"usage/module/#step-by-step-script","title":"\ud83e\ude9c Step by step script","text":"<p>To use the script as a module, you only need to import the class <code>PerformanceAssessor()</code> as so:</p> <pre><code>from perfassess.class_performance_assessor import PerformanceAssessor\n\n...\n</code></pre> <p>You can also, for testing, import <code>testor()</code> as so:</p> <pre><code>...\n\nfrom perfassess.testor import testor\n\n...\n</code></pre> <p>You can now launch the class on <code>testor()</code> for instance:</p> <pre><code>...\n\n\nassessor: PerformanceAssessor = PerformanceAssessor(\n    main=testor,\n    n_field=1,\n    value=[0] * 1000\n)\n\nassessor.launch_profiling()\nassessor.plot(path=\"output_directory/\")\n</code></pre> <p>Do not forget to change <code>\ud83d\udcc1 output_directory/</code> as your wish!</p>"},{"location":"usage/module/#full-test-script","title":"\ud83e\uddea Full test script","text":"<pre><code>from perfassess.class_performance_assessor import PerformanceAssessor\nfrom perfassess.testor import testor\n\nassessor: PerformanceAssessor = PerformanceAssessor(\n    main=testor,\n    n_field=1,\n    value=[0] * 1000\n)\n\nassessor.launch_profiling()\nassessor.plot(path=\"output_directory/\")\n</code></pre>"}]}